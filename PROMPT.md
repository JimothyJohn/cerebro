# Problem

It’s extremely costly and cumbersome to utilize vision perception models for niche use cases at the edge and I hope to make this easier, faster, cheaper, and more accessible by offering an efficient model accelerated in a cloud deployment. There’s a deeper problem here, but this is the technical one

# Goal

Create Cerebro, the name of a project that is a [Replicate](https://replicate.com) A100 endpoint for a CUDA-accelerated version of Microsoft's multi-modal Phi-3 model which is currently hosted on Huggingface at https://huggingface.co/microsoft/Phi-3.5-vision-instruct/tree/main

# Stakeholders

The designer of this architecture is an automation engineer specializing in deploying AI endpoints for scale. They have experience with Debian, Docker, CUDA, Cog, and Python. The user of the endpoint will be an enterprise user trying to gain insight into images by prompting with hints about what is in the image in an attempt to get a structured output with key details about what they need to know.

# Requirements

- Make it clear in all files that they are AI-generated by the current model being used (o1, Sonnet, 4o, Mistral, Gemini, Phi, etc.) so the user knows to check for errors.
- The program should be easy to follow and well-commented.
- The program will provide detailed logging to help troubleshoot during development.
- When providing code provide the entire program file, not just snippets.
- Make it as generalizable as possible in case I want to deploy other models or methods.
- Keep all of the program explanations inside of the files themselves as comments.
- Include error handling whenever possible.
- Download the model weights during image creation inside of the cog.yaml file, not during the setup at program runtime.
    - Store the models into the folder /model-cache
- Use containerization whenever possible to improve modularity
- The user is expected to provide a Replicate API key in their environment
- Recommend the most stable versions possible as of your knowledge cutoff date for maximum alignment.

# Outputs (in order of creation)

The expected folder structure should be:

```
├── cerebro.code-workspace
├── cog.yaml
├── cerebro
│   ├── predict.py
│   └── utils.py
├── docs
│   ├── cerebro.png
│   ├── index.html
│   └── style.css
├── requirements.txt
└── tests
    ├── deployment
    │   ├── test_deployment.py
    │   └── __init__.py
    ├── integration
    │   ├── test_integration.py
    │   └── __init__.py
    └── unit
        ├── __init__.py
        └── test_unit.py
```

### requirements.txt

A file of the library dependencies

```
Pillow
packaging
requests
transformers
torch
torchvision
cog
pytest
accelerate
pyreadline3
```

### predict.py

A [predict.py](http://predict.py) program that functions as the main inference program. Here’s an example:

```python
# predict.py
# Prediction interface for Cog ⚙️
# https://github.com/replicate/cog/blob/main/docs/python.md
"""
This program is for Phi-3.5-vision-instruct, the multi-modal generation of Phi models.
AI-generated code, please review for correctness.
"""

from cog import BasePredictor, Input
import os
import time
import torch
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor
import requests
import logging
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# MODEL_NAME = "microsoft/Phi-3.5-vision-instruct"
MODEL_CACHE = "/model-cache"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def is_ampere_or_newer() -> bool:
    if torch.cuda.is_available():
        capability = torch.cuda.get_device_capability()
        return capability[0] >= 8  # Ampere is compute capability 8.0+
    return False

class Predictor(BasePredictor):
    model: AutoModelForCausalLM
    processor: AutoProcessor

    def setup(self) -> None:
        """Load the model into memory to make running multiple predictions efficient"""
        if is_ampere_or_newer():
            torch.backends.cudnn.benchmark = True  # Optimize CuDNN kernel selection
            torch.backends.cudnn.enabled = True  # Ensure CuDNN is enabled

        start_time = time.time()
        logger.info("Setting up the model...")
        try:
            # Load the model from the cache directory
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_CACHE,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                _attn_implementation=(
                    "flash_attention_2" if is_ampere_or_newer() else "eager"
                ),  # or 'eager' if flash_attn not installed
            )

            # self.model = torch.jit.script(self.model)  # For dynamic control flow

            # For best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.
            self.processor = AutoProcessor.from_pretrained(
                MODEL_CACHE,
                trust_remote_code=True,
                # Experiment with lower numbers for faster processing
                num_crops=4,  # multi-frame
                # num_crops=16  # single-frame, results in torch.OutOfMemoryError: CUDA out of memory
            )
            logger.info(f"Model loaded in {time.time() - start_time:.2f} seconds")
        except Exception as e:
            logger.error(f"Error during model setup: {e}")
            raise e

    @torch.inference_mode()
    def predict(
        self,
        image_urls: str = Input(description="Comma-separated URLs of images"),
        prompt: str = Input(description="Input prompt"),
        max_new_tokens: int = Input(
            description="Max new tokens", default=1024, ge=64, le=2048
        ),
        temperature: float = Input(
            description="Temperature for generation", default=0.7, ge=0.0, le=1.0
        ),
        do_sample: bool = Input(
            # Set to False if enough VRAM is available
            description="Whether or not to use sampling; use greedy decoding otherwise.",
            default=True,
        ),
    ) -> str:
        """Run a single prediction on the model"""
        try:
            # Process images
            images: List[Image.Image] = []
            placeholder: str = ""
            image_url_list: List[str] = [url.strip() for url in image_urls.split(",")]
            for idx, url in enumerate(image_url_list):
                response: requests.Response = requests.get(url, stream=True)
                if response.status_code == 200:
                    image = Image.open(response.raw).convert("RGB")
                    images.append(image)
                    placeholder += f"<|image_{idx+1}|>\n"
                else:
                    logger.warning(f"Failed to retrieve image from URL: {url}")
            if not images:
                raise ValueError("No valid images were provided.")

            # Prepare messages
            messages: List[Dict[str, str]] = [
                {"role": "user", "content": placeholder + prompt},
            ]

            # Prepare prompt
            prepared_prompt: str = self.processor.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # Prepare inputs
            inputs: Dict[str, Any] = self.processor(
                prepared_prompt, images=images, return_tensors="pt"
            ).to(DEVICE)

            # Generation arguments
            generation_args: Dict[str, Any] = {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": do_sample,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
            }

            # Generate output
            outputs: torch.Tensor = self.model.generate(**inputs, **generation_args)

            # Remove input tokens
            generated_tokens: torch.Tensor = outputs[:, inputs["input_ids"].shape[1] :]

            # Decode response
            response: str = self.processor.batch_decode(
                generated_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )[0]

            return response

        except Exception as e:
            logger.error(f"Error during prediction: {e}")
            raise e

```

tests/ Basic unit tests built using the pytest framework

### cog.yaml

A cog.yaml file that will serve as the configuration file for the container. The cog.yaml should include run instructions that download the model into the container image so it doesn’t have to be downloaded by predict.py. Here is an example:

```yaml
build:
  gpu: true
  system_packages:
    - "git"
    - "git-lfs"
  python_version: "3.11"
  python_requirements: "requirements.txt"
  run:
    # https://github.com/Dao-AILab/flash-attention/issues/453
    - pip install flash-attn --no-build-isolation
    # https://onnxruntime.ai/docs/genai/tutorials/phi3-v.html#run-with-nvidia-cuda
    # - pip install onnxruntime-genai-cuda --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/
    - git lfs install
    - git clone https://huggingface.co/microsoft/Phi-3.5-vision-instruct /model-cache
    - cd /model-cache && git lfs pull

predict: "cerebro/predict.py:Predictor"

image: "r8.im/jimothyjohn/phi3-vision-instruct"
```

index.html and style.css - An HTML version of the README that can be used in conjunction with Github Pages along with CSS for readability and formatting.

.gitignore - A .gitignore file suitable for this type of Python-Cog deployment

# Error Handling and Testing

- Ensure the user to provide an API key in their request - Create an error when none is provided or when Replicate says the one being provided is invalid.
- Make sure an image is sent to the API
- Make sure text is also provided with the image
- Make sure the endpoint format is as expected
- Make sure input format is as expected
- More stuff that would be typical in an API
- Add tests that send a few prompts to the model and expect a specific answer. All of the prompts will use the same image. Here are the prompts:
    - Tell me how many people are in this image. Your answer should be just a single number, for example "2". When I ask you how many people are in the image you must only output a number with no additional text. For example if I asked you how many people were in an image and there were 2 people you would only respond with "2".
    - Count the number of subjects in the image. There will be an obvious background and foreground, and you will tell me how many objects you see in the foreground. Your answer should be just a single number, for example "2". When I ask you how many people are in the image you must only output a number with no additional text. For example if I asked you how many people were in an image and there were 2 people you would only respond with "2".

# Examples

```python
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License
# Inference code for Phi-3-vision-128k-instruct-onnx-cuda
# https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3v.py

import argparse
import os
import readline
import glob

import onnxruntime_genai as og

def _complete(text, state):
    return (glob.glob(text + "*") + [None])[state]

def run(args: argparse.Namespace):
    print("Loading model...")
    model = og.Model(args.model_path)
    processor = model.create_multimodal_processor()
    tokenizer_stream = processor.create_stream()

    while True:
        readline.set_completer_delims(" \t\n;")
        readline.parse_and_bind("tab: complete")
        readline.set_completer(_complete)
        image_paths = [
            image_path.strip()
            for image_path in input(
                "Image Path (comma separated; leave empty if no image): "
            ).split(",")
        ]
        image_paths = [image_path for image_path in image_paths if len(image_path)]
        print(image_paths)

        images = None
        prompt = "<|user|>\n"
        if len(image_paths) == 0:
            print("No image provided")
        else:
            print("Loading images...")
            for i, image_path in enumerate(image_paths):
                if not os.path.exists(image_path):
                    raise FileNotFoundError(f"Image file not found: {image_path}")
                prompt += f"<|image_{i+1}|>\n"

            images = og.Images.open(*image_paths)

        text = input("Prompt: ")
        prompt += f"{text}<|end|>\n<|assistant|>\n"
        print("Processing images and prompt...")
        inputs = processor(prompt, images=images)

        print("Generating response...")
        params = og.GeneratorParams(model)
        params.set_inputs(inputs)
        params.set_search_options(max_length=7680)

        generator = og.Generator(model, params)

        while not generator.is_done():
            generator.compute_logits()
            generator.generate_next_token()

            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end="", flush=True)

        for _ in range(3):
            print()

        # Delete the generator to free the captured graph before creating another one
        del generator

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-m", "--model_path", type=str, required=True, help="Path to the model"
    )
    args = parser.parse_args()
    run(args)
```

```python
# Sample code for Phi3.5-vision-instruct inference
# https://huggingface.co/microsoft/Phi-3.5-vision-instruct#loading-the-model-locally
from PIL import Image 
import requests 
from transformers import AutoModelForCausalLM 
from transformers import AutoProcessor 

model_id = "microsoft/Phi-3.5-vision-instruct" 

# Note: set _attn_implementation='eager' if you don't have flash_attn installed
model = AutoModelForCausalLM.from_pretrained(
  model_id, 
  device_map="cuda", 
  trust_remote_code=True, 
  torch_dtype="auto", 
  _attn_implementation='flash_attention_2'    
)

# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.
processor = AutoProcessor.from_pretrained(model_id, 
  trust_remote_code=True, 
  num_crops=4
) 

images = []
placeholder = ""

# Note: if OOM, you might consider reduce number of frames in this example.
for i in range(1,20):
    url = f"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg" 
    images.append(Image.open(requests.get(url, stream=True).raw))
    placeholder += f"<|image_{i}|>\n"

messages = [
    {"role": "user", "content": placeholder+"Summarize the deck of slides."},
]

prompt = processor.tokenizer.apply_chat_template(
  messages, 
  tokenize=False, 
  add_generation_prompt=True
)

inputs = processor(prompt, images, return_tensors="pt").to("cuda:0") 

generation_args = { 
    "max_new_tokens": 1000, 
    "temperature": 0.0, 
    "do_sample": False, 
} 

generate_ids = model.generate(**inputs, 
  eos_token_id=processor.tokenizer.eos_token_id, 
  **generation_args
)

# remove input tokens 
generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
response = processor.batch_decode(generate_ids, 
  skip_special_tokens=True, 
  clean_up_tokenization_spaces=False)[0] 

print(response)

```

# Conclusion

After all of the above outputs have been created:

- Provide step by step instructions showing how to deploy the code as a new Github repository using bash in a Unix-like terminal.
- Provide suggestions for additional functionality and enhancements after the outputs
- Recommend adjustments to the prompt to better refine results
- Point out areas of ambiguity or confusion so I can add clarification
- Provide the file structure expected at this point in the conversation if it’s changed since the last step.