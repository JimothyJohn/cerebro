### Problem

It’s extremely costly and cumbersome to utilize vision perception models for niche use cases at the edge and I hope to make this easier, faster, cheaper, and more accessible by offering an efficient model accelerated in a cloud deployment. There’s a deeper problem here, but this is the technical one

### Goal

Create Cerebro, the name of a project that is a [Replicate](https://replicate.com) A100 endpoint for a CUDA-accelerated version of Microsoft's multi-modal Phi-3 model which is currently hosted on Huggingface at https://huggingface.co/microsoft/Phi-3.5-vision-instruct/tree/main

### Stakeholders

The designer of this architecture is an automation engineer specializing in deploying AI endpoints for scale. They have experience with Debian, Docker, CUDA, Cog, and Python. The user of the endpoint will be an enterprise user trying to gain insight into images by prompting with hints about what is in the image in an attempt to get a structured output with key details about what they need to know.

### Requirements

- Make it clear in all files that they are AI-generated by the current model being used (o1, Sonnet, 4o, Mistral, Gemini, Phi, etc.) so the user knows to check for errors.
- The program should be easy to follow and well-commented.
- The program will provide detailed logging to help troubleshoot during development.
- When providing code provide the entire program file, not just snippets.
- Make it as generalizable as possible in case I want to deploy other models or methods.
- Keep all of the program explanations inside of the files themselves as comments.
- Include error handling whenever possible.
- Download the model weights during image creation inside of the cog.yaml file, not during the setup at program runtime.
    - Store the models into the folder /model-cache

### Outputs (in order of creation)

### requirements.txt

A file of the library dependencies

```
Pillow
packaging
requests
transformers
onnxruntime-gpu
torch
cog
pytest
accelerate
pyreadline3
```

### predict.py

A [predict.py](http://predict.py) program that functions as the main inference program. Here’s an example using the Phi-2 non-vision version:

```python
# predict.py
# Prediction interface for Cog ⚙️
# https://github.com/replicate/cog/blob/main/docs/python.md
"""
This program is a for Phi-2, the previous, text-only generation of Phi models.
"""

from cog import BasePredictor, Input
import time
import torch
from weights_downloader import WeightsDownloader
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_NAME = "microsoft/phi-2"
MODEL_CACHE = "model-cache"
MODEL_URL = "https://weights.replicate.delivery/default/microsoft/phi-2.tar"

class Predictor(BasePredictor):
    def setup(self) -> None:
        """Load the model into memory to make running multiple predictions efficient"""
        start = time.time()
        WeightsDownloader.download_if_not_exists(MODEL_URL, MODEL_CACHE)

        print("Loading pipeline...")
        torch.set_default_device("cuda")
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype="auto",
            trust_remote_code=True,
            cache_dir=MODEL_CACHE
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        print("setup took: ", time.time() - start)

    @torch.inference_mode()
    def predict(
        self,
        prompt: str = Input(description="Input prompt"),
        max_length: int = Input(
            description="Max length", ge=0, le=2048, default=200
        ),
    ) -> str:
        """Run a single prediction on the model"""
        inputs = self.tokenizer(prompt, return_tensors="pt", return_attention_mask=False)
        outputs = self.model.generate(**inputs, max_length=max_length)
        result = self.tokenizer.batch_decode(outputs)[0]

        return result
```

[tests.py](http://tests.py) - Basic unit tests built using the pytest framework

### cog.yaml

A cog.yaml file that will serve as the configuration file for the container. The cog.yaml should include run instructions that download the model into the container image so it doesn’t have to be downloaded by predict.py. Here is a Phi-2 non-vision example:

```yaml
# Configuration for Cog ⚙️
# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md

build:
  # set to true if your model requires a GPU
  gpu: true
  system_packages:
    - "git"
    - "git-lfs"
  python_version: "3.11"
  python_requirements: "requirements.txt"

  run:
	  # https://github.com/Dao-AILab/flash-attention/issues/453
    - pip install flash-attn --no-build-isolation
    - curl -o /usr/local/bin/pget -L "https://github.com/replicate/pget/releases/download/v0.3.1/pget" && chmod +x /usr/local/bin/pget

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"
```

index.html and style.css - An HTML version of the README that can be used in conjunction with Github Pages along with CSS for readability and formatting.

.gitignore - A .gitignore file suitable for this type of Python-Cog deployment

devcontainer.json - A fully-featured development container file for VS Code that makes development instantaneous and lends itself well to Github Codespaces for cloud development

[PROMPT.](http://PROMPT.md)txt - Saves this prompt as a plaintext file in a more readable and easily applicable format.

### README.md

[README.md](http://README.md) - A beautiful markdown file that elegantly explains what this repository does.

```markdown
# Cerebro - Dawn of Perception

This repository provides a setup for deploying Microsoft's multi-modal Phi-3 model using ONNX Runtime with CUDA acceleration. The goal is to create a Replicate A100 endpoint for efficient cloud deployment, making it easier, faster, and more accessible to utilize vision perception models for niche use cases.

## Features

- **CUDA Acceleration**: Leverages CUDA for faster inference on GPUs.
- **Easy Deployment**: Configurations are provided for quick setup using VS Code or GitHub Codespaces.

## Getting Started

### Prerequisites

- NVIDIA GPU with CUDA support
- Docker installed (for devcontainer)
- VS Code with Remote Containers extension (optional)

### Setup Instructions

1. **Clone the Repository**

   ```bash
   git clone https://github.com/JimothyJohn/cerebro.git
   cd phi-3-deployment
   ```

2. **Using Dev Container**
    
    * Open the repository in VS Code.
    
    * When prompted, "Reopen in Container" to use the devcontainer setup.
    
    * Alternatively, you can use GitHub Codespaces for a cloud-based development environment.

3. **Install Dependencies**

    * If not using the devcontainer, create a virtual environment and install dependencies:

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

4. **Run Tests**

    * Ensure everything is set up correctly by running the unit tests:

```bash
pytest tests.py
```

5. **Running Inference**

    * You can run the predict.py script directly or set it up as part of a Cog deployment.

## Usage

The predict.py script accepts image URLs and a text prompt to generate a response using the Phi-3 model.

Example:

```py
from predict import Predictor

predictor = Predictor()
predictor.setup()
response = predictor.predict(
    image_urls="https://example.com/image1.jpg, https://example.com/image2.jpg",
    prompt="Summarize the content of these images.",
    max_new_tokens=100,
    temperature=0.5,
    do_sample=True
)

print(response)
```

### Notes

* AI-Generated Code: Most of the code in this repository is AI-generated and should be reviewed for correctness and compatibility.

* Model Files: The model files are downloaded automatically if not present in the model-cache directory.

### Contributing

Contributions are welcome! Please submit a pull request or open an issue for any changes or suggestions.

### License - MIT

```

### Examples

```python
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License
# Inference code for Phi-3-vision-128k-instruct-onnx-cuda
# https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3v.py

import argparse
import os
import readline
import glob

import onnxruntime_genai as og

def _complete(text, state):
    return (glob.glob(text + "*") + [None])[state]

def run(args: argparse.Namespace):
    print("Loading model...")
    model = og.Model(args.model_path)
    processor = model.create_multimodal_processor()
    tokenizer_stream = processor.create_stream()

    while True:
        readline.set_completer_delims(" \t\n;")
        readline.parse_and_bind("tab: complete")
        readline.set_completer(_complete)
        image_paths = [
            image_path.strip()
            for image_path in input(
                "Image Path (comma separated; leave empty if no image): "
            ).split(",")
        ]
        image_paths = [image_path for image_path in image_paths if len(image_path)]
        print(image_paths)

        images = None
        prompt = "<|user|>\n"
        if len(image_paths) == 0:
            print("No image provided")
        else:
            print("Loading images...")
            for i, image_path in enumerate(image_paths):
                if not os.path.exists(image_path):
                    raise FileNotFoundError(f"Image file not found: {image_path}")
                prompt += f"<|image_{i+1}|>\n"

            images = og.Images.open(*image_paths)

        text = input("Prompt: ")
        prompt += f"{text}<|end|>\n<|assistant|>\n"
        print("Processing images and prompt...")
        inputs = processor(prompt, images=images)

        print("Generating response...")
        params = og.GeneratorParams(model)
        params.set_inputs(inputs)
        params.set_search_options(max_length=7680)

        generator = og.Generator(model, params)

        while not generator.is_done():
            generator.compute_logits()
            generator.generate_next_token()

            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end="", flush=True)

        for _ in range(3):
            print()

        # Delete the generator to free the captured graph before creating another one
        del generator

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-m", "--model_path", type=str, required=True, help="Path to the model"
    )
    args = parser.parse_args()
    run(args)
```

```python
# Sample code for Phi3.5-vision-instruct inference
# https://huggingface.co/microsoft/Phi-3.5-vision-instruct#loading-the-model-locally
from PIL import Image 
import requests 
from transformers import AutoModelForCausalLM 
from transformers import AutoProcessor 

model_id = "microsoft/Phi-3.5-vision-instruct" 

# Note: set _attn_implementation='eager' if you don't have flash_attn installed
model = AutoModelForCausalLM.from_pretrained(
  model_id, 
  device_map="cuda", 
  trust_remote_code=True, 
  torch_dtype="auto", 
  _attn_implementation='flash_attention_2'    
)

# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.
processor = AutoProcessor.from_pretrained(model_id, 
  trust_remote_code=True, 
  num_crops=4
) 

images = []
placeholder = ""

# Note: if OOM, you might consider reduce number of frames in this example.
for i in range(1,20):
    url = f"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg" 
    images.append(Image.open(requests.get(url, stream=True).raw))
    placeholder += f"<|image_{i}|>\n"

messages = [
    {"role": "user", "content": placeholder+"Summarize the deck of slides."},
]

prompt = processor.tokenizer.apply_chat_template(
  messages, 
  tokenize=False, 
  add_generation_prompt=True
)

inputs = processor(prompt, images, return_tensors="pt").to("cuda:0") 

generation_args = { 
    "max_new_tokens": 1000, 
    "temperature": 0.0, 
    "do_sample": False, 
} 

generate_ids = model.generate(**inputs, 
  eos_token_id=processor.tokenizer.eos_token_id, 
  **generation_args
)

# remove input tokens 
generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
response = processor.batch_decode(generate_ids, 
  skip_special_tokens=True, 
  clean_up_tokenization_spaces=False)[0] 

print(response)

```

### Conclusion

- Provide suggestions for additional functionality and enhancements after the outputs
- Recommend adjustments to the prompt to better refine results
- Point out areas of ambiguity or confusion so I can add clarification